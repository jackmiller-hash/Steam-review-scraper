{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPY7eJKXl+RggQuAAWJKUE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackmiller-hash/Steam-review-scraper/blob/master/Utils/Emergency_Processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82ZbDgaeaw8E"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ› ï¸ Emergency Data Processor { display-mode: \"form\" }\n",
        "#@markdown Run this if the scraper gets stuck in a \"Cooldown\" loop to process what you've already downloaded.\n",
        "\n",
        "import os, json, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "APP_ID = 1903340 #@param {type:\"integer\"}\n",
        "REPORT_NAME = \"Expedition33_Recovered_Report\" #@param {type:\"string\"}\n",
        "\n",
        "path = f'data/review_{APP_ID}.json'\n",
        "\n",
        "if os.path.exists(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    reviews = data.get('reviews', {})\n",
        "    print(f\"âœ… Found {len(reviews)} reviews. Starting Analysis...\")\n",
        "\n",
        "    results = []\n",
        "    for r_id in tqdm(reviews, desc=\"Processing Local Data\"):\n",
        "        text = reviews[r_id].get('review', '')\n",
        "        if text:\n",
        "            score = TextBlob(text).sentiment.polarity\n",
        "            results.append({'Review': text, 'Score': score})\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(f\"{REPORT_NAME}.csv\", index=False)\n",
        "\n",
        "    # Generate Clouds\n",
        "    pos_text = \" \".join(df[df['Score'] > 0.2]['Review'])\n",
        "    neg_text = \" \".join(df[df['Score'] < -0.1]['Review'])\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "    if pos_text:\n",
        "        wc_pos = WordCloud(background_color=\"white\", colormap=\"Greens\").generate(pos_text)\n",
        "        ax1.imshow(wc_pos, interpolation='bilinear'); ax1.axis(\"off\"); ax1.set_title(\"FAN FAVORITES\")\n",
        "    if neg_text:\n",
        "        wc_neg = WordCloud(background_color=\"white\", colormap=\"Reds\").generate(neg_text)\n",
        "        ax2.imshow(wc_neg, interpolation='bilinear'); ax2.axis(\"off\"); ax2.set_title(\"PAIN POINTS\")\n",
        "    plt.show()\n",
        "    print(f\"\\nDONE: Report saved as {REPORT_NAME}.csv\")\n",
        "else:\n",
        "    print(\"âŒ No local data found. You need to run the scraper at least once first.\")"
      ]
    }
  ]
}